import sys
from unittest.mock import patch

import pytest

from rasa_chinese_service.nlu.tokenizers.lm_tokenizer import (
    language_model_tokenizer_service,
)


@pytest.mark.parametrize(
    "text, expected_tokens, expected_indices",
    [
        (
            "æˆ‘æƒ³å»åƒå…°å·æ‹‰é¢",  # easy/normal case
            ["æˆ‘", "æƒ³", "å»", "åƒ", "å…°", "å·", "æ‹‰", "é¢"],
            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)],
        ),
        (
            "ä»ä¸œç•ˆæ‘èµ°äº†ã€‚",  # OOV case: `ç•ˆ` is a OOV word
            ["ä»", "ä¸œ", "[UNK]", "æ‘", "èµ°", "äº†", "ã€‚"],
            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7)],
        ),
        (
            "Micheal ä½ å¥½å—ï¼Ÿ",  # Chinese mixed up with English
            ["[UNK]", "ä½ ", "å¥½", "å—", "ï¼Ÿ"],
            [
                (0, 7),
                (8, 9),
                (9, 10),
                (10, 11),
                (11, 12),
            ],
        ),
        (
            "æˆ‘æƒ³ä¹° iPhone 12 ğŸ¤­",  # Chinese mixed up with English, numbers, and emoji
            ["æˆ‘", "æƒ³", "ä¹°", "[UNK]", "12", "[UNK]"],
            [(0, 1), (1, 2), (2, 3), (4, 10), (11, 13), (14, 15)],
        ),
    ],
)
def test_tokenizer_for_chinese(text, expected_tokens, expected_indices):
    with patch.object(
        sys,
        "argv",
        ["rasa_chinese_service.nlu.tokenizers.lm_tokenizer", "bert-base-chinese"],
    ):
        app = language_model_tokenizer_service()

        _, response = app.test_client.get("/", params={"q": text})
        tokens = response.json

        assert [t[0] for t in tokens] == expected_tokens
        assert [t[1] for t in tokens] == [i[0] for i in expected_indices]
        assert [t[2] for t in tokens] == [i[1] for i in expected_indices]
